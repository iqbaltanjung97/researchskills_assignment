{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bandit Environment\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])  # Bernoulli rewards\n",
    "\n",
    "# ==========================\n",
    "# Fixed Exploration Then Exploitation (TODO: Implement switching strategy)\n",
    "# ==========================\n",
    "class FixedExplorationThenGreedy:\n",
    "    def __init__(self, num_arms, exploration_steps):\n",
    "        self.num_arms = num_arms\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement fixed exploration for N steps, then greedy \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy Algorithm (TODO: Complete update function)\n",
    "# ==========================\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, num_arms, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement incremental mean update \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy with Decaying Exploration (TODO: Complete decay schedule and compare schedules)\n",
    "# ==========================\n",
    "class EpsilonGreedyDecaying:\n",
    "    def __init__(self, num_arms, epsilon_schedule):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon_schedule = epsilon_schedule  # Function for epsilon_t\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule including epsilon decay \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# UCB Algorithm (TODO: Complete selection function)\n",
    "# ==========================\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement UCB selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Thompson Sampling Algorithm (TODO: Implement Thompson Sampling)\n",
    "# ==========================\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.successes = np.zeros(num_arms)\n",
    "        self.failures = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement Thompson Sampling selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule for Beta distribution \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# Experiment Runner\n",
    "# ==========================\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# ==========================\n",
    "# Running Experiments\n",
    "# ==========================\n",
    "num_arms = 10\n",
    "reward_means = np.linspace(0, 1, num_arms)  # Linearly spaced rewards\n",
    "env = BanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Define epsilon schedule\n",
    "def epsilon_schedule(t):\n",
    "    return 1 / (t + 1)\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Run and plot Fixed Exploration Then Exploitation\n",
    "fixed_exploration_regret = run_experiment(FixedExplorationThenGreedy, {'num_arms': num_arms, 'exploration_steps': 100}, env, num_steps)\n",
    "plt.plot(fixed_exploration_regret, label='Fixed Exploration')\n",
    "\n",
    "# Run and plot ε-Greedy\n",
    "epsilon_greedy_regret = run_experiment(EpsilonGreedy, {'num_arms': num_arms, 'epsilon': 0.1}, env, num_steps)\n",
    "plt.plot(epsilon_greedy_regret, label='Epsilon-Greedy')\n",
    "\n",
    "# Run and plot Decaying ε-Greedy\n",
    "decaying_epsilon_greedy_regret = run_experiment(EpsilonGreedyDecaying, {'num_arms': num_arms, 'epsilon_schedule': epsilon_schedule}, env, num_steps)\n",
    "plt.plot(decaying_epsilon_greedy_regret, label='Decaying Epsilon-Greedy')\n",
    "\n",
    "# Run and plot UCB\n",
    "ucb_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 4}, env, num_steps)\n",
    "plt.plot(ucb_regret, label='UCB')\n",
    "\n",
    "# Run and plot Thompson Sampling\n",
    "thompson_regret = run_experiment(ThompsonSampling, {'num_arms': num_arms}, env, num_steps)\n",
    "plt.plot(thompson_regret, label='Thompson Sampling')\n",
    "\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Bandit Algorithm Performance\")\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# Instructions for Students\n",
    "# ==========================\n",
    "print(\"TODO: Complete the missing functions for Fixed-Exploration-Greedy, Epsilon-Greedy, Decaying Epsilon-Greedy, UCB, and Thompson Sampling.\")\n",
    "print(\"TODO: Implement and compare different epsilon schedules (e.g., 1/t, 1/sqrt(t), log(t)/t). Discuss the impact on exploration and cumulative regret in your report.\")\n",
    "print(\"TODO: Answer the questions in the assignment and conduct the necessary experiments to answer them.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
